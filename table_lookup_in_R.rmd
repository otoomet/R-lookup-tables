---
title: "Lookup Tables in R"
subtitle: "Different Approaches"
author: Ott Toomet
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: 
  html_document:
    highlight: tango
    df_print: kable
    fig_caption: false
---

# Setup

What is the most efficient way to create lookup tables in R?  I test a
few approaches for relatively large tables is approximately 100,000
rows by 50 columns.  The main trick is that the rows are indexed by
numbers, and columns by dates--I cannot just use ordinary matrix
indexing but have first to locate the correct row and column from an 1D
table.  Second, I am interested in repeated look-ups of single
elements in arbitrary order.  So no full row or full column
extractions.  These are not a huge tables nowadays, but as R does not
have a dedicated Map data type, I was not sure what is the best way to
quickly look up values from such a table.

We time this using the following number of rows:
```{r setup, echo=FALSE}
knitr::opts_knit$set(aliases=c(h="fig.height"))
knitr::opts_chunk$set(fig.height=100/25.4, 
                     message=FALSE,
                     cache=TRUE,
                     cache.path=".table_lookup_cache/",
                     engine.path = list(python = "/usr/bin/python3")
                     )
library(foreach)  # for convenient loops
library(magrittr) # 'cause I love pipes
library(ggplot2)
```

```{r R}
n <- c(1e3, 3e3, 1e4, 3e4, 1e5, 3e5)
                           # timing repetitions used later
n
```

# The Data

Now let's create the data.  As I am not concerned with the efficiency
of the data generation, I just create a large matrix here, and return
the row and column values as attributes.  This code is fast as it is fully
vectorized.

```{r matrixDGP}
DGP <- function(n1=120000, n2=50) {
   ## create matrix of n1 msisdns by n2 dates
   ## content is random integers
   numbers <- as.numeric(sample(1e9, n1))
   dates <- seq(from=as.Date("2011-08-01"), by="month", length=n2)
   tab <- matrix(sample(1000, n1*n2, replace=TRUE), n1, n2)
   attr(tab, "rows") <- numbers
   attr(tab, "cols") <- dates
   tab
}
```


# 0. Benchmark: no lookup

To get an idea how fast lookup times can we realistically expect, I
first run the code that does not contain any lookup, just extracting values
from the matrix by known indices.
```{r noMatch, cache=TRUE}
noMatch <- function(data) {
   rows <- attr(data, "rows")
   cols <- attr(data, "cols")
   for(i in seq(along=rows)) {
      for(j in seq(length=length(cols))) {
         a <- data[i,j]
      }
   }
}
```
and let's time it with 6 different sizes of data:
```{r noMatchMatch, cache=TRUE, dependson=c("R", "noMatch")}
time <- foreach(n1 = n, .combine=c) %do% {
   dat <- DGP(n1)
   system.time(noMatch(dat))["user.self"] %>%
      set_names(n1)
}
time
```
So, just extracting elements from such a matrix with 100k rows takes approximately
1s (on an i7-3440 system).  We can also see a close to linear growth,
10x larger matrix takes approximately 10 time more time to go through.

```{r, include=FALSE, dependson=c("R", "noMatchMatch")}
results <- data.frame(n=n, noMatch=time)
```


# 1. `match` by row and column values

The first real task is to try the most straightforward approach by using
`match(value, rowValues)` to find row and `match(value, colValues)` to
find the column:
```{r matrixMatch}
matrixMatch <- function(data) {
   rows <- attr(data, "rows")
   cols <- attr(data, "cols")
   for(number in sample(rows)) {
      for(date in sample(cols)) {
         i <- match(number, rows)
         j <- match(date, cols)
         a <- data[i,j]
      }
   }
}
```
The function extracts row and column values from the data attributes,
and thereafter looks up
the values from the matrix in an arbitrary order.  As I am only
concerned with the lookup speed itself, I discard the result.
```{r timeMatrixMatch, cache=TRUE, dependson=c("R", "matrixMatch")}
time <- foreach(n1 = n, .combine=c) %do% {
   dat <- DGP(n1)
   system.time(matrixMatch(dat))["user.self"] %>%
      set_names(n1)
}
time
```

```{r, include=FALSE}
results$matrixMatch <- time
```
`match` is slow for large tables.  The largest one reported here (15M
entries) takes 1hr 46 minutes to churn through.  This is probably too
slow for many realistic applications.  Note that running time grows
roughly a n^2, suggesting `match` uses just sequential comparisons.


# 2. Fastmatch' `fmatch` by row and column values

Our next approach is to use the function `fmatch` from
[`fastmatch` package](https://cran.r-project.org/web/packages/fastmatch/index.html).
At the first pass, `fmatch` creates a hash table, and uses that table
on subsequent passes.
We keep the rest of the code identical to that in the previous section:
```{r matrixFmatch}
matrixFmatch <- function(data) {
   rows <- attr(data, "rows")
   cols <- attr(data, "cols")
   for(number in sample(rows)) {
      for(date in sample(cols)) {
         i <- fastmatch::fmatch(number, rows)
         j <- fastmatch::fmatch(date, cols)
         a <- data[i,j]
      }
   }
}
```
And the results are:
```{r timeMatrixFmatch, cache=TRUE, dependson=c("matrixFmatch", "R")}
time <- foreach(n1 = n, .combine=c) %do% {
   dat <- DGP(n1)
   system.time(matrixFmatch(dat))["user.self"] %>%
      set_names(n1)
}
time
```
We see a large improvement in the access time for larger
tables--apparently the initial hash table creation dominates the
timing for less than 10,000 records.  We are also back to the linear
growth in the lookup-time.


```{r, include=FALSE}
results$matrixFmatch <- time
```

# 3. Mixed `fmatch` and `match`

As `fmatch` appears to be slow for small tables, we can replace
matching by the small number of columns by `match`, keeping the code
otherwise as above.

```{r matrixMixedMatch}
matrixMixedMatch <- function(data) {
   rows <- attr(data, "rows")
   cols <- attr(data, "cols")
   for(number in sample(rows)) {
      for(date in sample(cols)) {
         i <- fastmatch::fmatch(number, rows)
         j <- match(date, cols)
         a <- data[i,j]
      }
   }
}
```
And the results are:
```{r timeMatrixMixedMatch, cache=TRUE, dependson=c("matrixMixedMatch", "R")}
time <- foreach(n1 = n, .combine=c) %do% {
   dat <- DGP(n1)
   system.time(matrixMixedMatch(dat))["user.self"] %>%
      set_names(n1)
}
time
```
We again see a noticeable improvement.  Apparently switching from
`fmatch` to `match` pays off for small tables.  As it also involves
one less letter of typing, I will recommend this trick for everyone.
Note that this move cut the lookup time of pure `fmatch` down to 56%
preserving the linear growth.

```{r, include=FALSE}
results$matrixMixedMatch <- time
```

# 4. Hashed environments

So far I have created data as matrices.  But R offers other ways.  For
instance, we can store data in nested environments.  As we cannot
vectorize environment access, the approach is slow, but this post only
considers lookup-time, not table creation time.
```{r envDGP}
envDGP <- function(n1=120000, n2=50) {
   ## create matrix of n1 msisdns by n2 dates
   ## content is random integers
   numbers <- as.numeric(sample(1e9, n1))
   numberNames <- paste0("x", numbers)
   dates <- seq(from=as.Date("2011-08-01"), by="month", length=n2)
   dateNames <- paste0("d", format(dates, "%Y%m%d"))
   dataEnv <- new.env(hash=TRUE)
   for(id in seq(length=length(dates))) {
      dateEnv <- new.env()
      i <- sample(1000, length(numbers), replace=TRUE)
      lapply(seq(along=numbers), function(j) dateEnv[[numberNames[j] ]] <- i[j])
      dataEnv[[dateNames[id] ]] <- dateEnv
   }
   attr(dataEnv, "rows") <- numbers
   attr(dataEnv, "cols") <- dates
   dataEnv
}
```
We use the following extraction function:

```{r nestedEnv}
nestedEnv <- function(data) {
   ## data: environment for nested data
   rows <- attr(data, "rows")
   cols <- attr(data, "cols")
   numberNames <- paste0("x", rows)
   dateNames <- paste0("d", format(cols, "%Y%m%d"))
   for(number in sample(numberNames)) {
      for(date in sample(dateNames)) {
         a <- data[[date]][[number]]
      }
   }
}
```
And the results are:
```{r timeNestedEnv, cache=TRUE, dependson=c("nestedEnv", "R")}
time <- foreach(n1 = n, .combine=c) %do% {
   dat <- envDGP(n1)
   system.time(nestedEnv(dat))["user.self"] %>%
      set_names(n1)
}
time
```
Now we see a dramatic, almost 10-fold improvement over the mixed
matching approach above for the middle-sized data.  For large data,
however, the advantage is not as big.  The growth rate is faster than
linear but not quite quadratic either.

```{r, include=FALSE}
results$nestedEnv <- time
```


# Final plot

Finally, here is a plot with all approaches together.

```{r plotAll, echo=FALSE}
tidyr::gather(results, key=method, value=time, noMatch:nestedEnv) %>%
   ggplot(aes(n, time, col=method)) +
   geom_line() + geom_point() +
   scale_x_log10() + scale_y_log10() +
   labs(y = "time, s")
```

R's built-in `match` is clearly the slowest approach for anything like
large tables.  For the sizes analyzed here, environments offer the
best speed albeit at a cost of some more complexity.


# For comparison: the same process in python/pandas



```{python}
#
import pandas as pd
import numpy as np
import time
def dgp(n1, n2):
    numbers = list(np.random.choice(int(1e7), n1, replace=False))
    # use a smaller number to choose from here -- seems to be terribly slow otherwise
    dates = list(pd.date_range('2011-03-04', periods=n2))
    data = pd.DataFrame(np.random.choice(1000, size=(n1,n2)),
                        columns=dates, index=numbers)
    return data, numbers, dates
def extract(data, rows, columns):
    t0 = time.time()
    for number in np.random.choice(rows, size=len(rows), replace=False):
        for date in np.random.choice(columns, size=len(columns), replace=False):
            i = rows.index(number)
            j = columns.index(date)
            a = data.iloc[i,j]
    t1 = time.time()
    print(data.shape[0], "\n", t1 - t0)
n2 = 50
n1s = np.array([1e3, 3e3, 1e4, 3e4, 1e5, 3e5], dtype="int")
n1s = np.array([1e3, 3e3, 1e4], dtype="int")
print("n\tseconds")
for n1 in n1s:
    (data, rows, columns) = dgp(n1, n2)
    extract(data, rows, columns)
```

Why is it so much slower?
