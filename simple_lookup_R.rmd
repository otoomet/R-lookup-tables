---
title: "Simple Int-to-Int Value Lookup R"
subtitle: "Different Approaches"
author: Ott Toomet
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: 
  html_document:
    highlight: tango
    df_print: kable
    fig_caption: false
    code_folding: hide
---

# Setup

In [an earlier post](https://otoomet.github.io/table_lookup_in_R.html)
I compared the time for looking up values in a 2D 
table, indexed by different type of objects.  Now let's take a step
backward and analyze a simple 1D value lookup.  I am focusing on
integer-integer lookup, i.e. both keys and values are integers.  The
type of values is probably of little importance as most of the work is presumably
done looking up the keys in the key tables.  However, I model it in a bit more complex way to
see if different data types make any difference.

The tests are done on `bash lscpu | grep "Model name:" | sed -e
"s/Model name: \+//"` CPU on a single core.

We create pairs of vectors to keep the values, and use the first one
to look up the value in the second one.


```{r setup, echo=FALSE}
knitr::opts_knit$set(aliases=c(h="fig.height"))
knitr::opts_chunk$set(fig.height=100/25.4, 
                     message=FALSE,
                     cache=TRUE,
                     cache.path=".table_lookup_cache/",
                     engine.path = list(python = "/usr/bin/python3")
                     )
library(foreach)  # for convenient loops
library(magrittr) # 'cause I love pipes
library(ggplot2)
```

We look up all the values of tables of different sizes:

```{r R}
n <- c(1e1, 3e1, 1e2, 3e2, 1e3, 3e3, 1e4, 3e4, 1e5, 3e5)
                           # timing repetitions used later
n
```

We use _microbenchmark_ library and report timing in milliseconds.


## Benchmark: no lookup

First we do the exercise with no actual value lookup.  This serves as
a benchmark: in particular for small lookup tables, the actual lookup
time may be small compared to other related overheads.  We just loop
over all keys and pick the value corresponding to that key, using
positional indexing.

```{r noLookup, dependson="R"}
doNothing <- function(keys, values) {
   ## find values, corresponding to keys
   for(key in keys) {
      v <- values[key]
   }
}

time <- foreach(n1 = n, .combine=c) %do% {
   values <- sample(1000000000L, n1)
   keys <- sample(seq(along=values))
                           # keys are just the integer indices, in random order
   microbenchmark::microbenchmark(doNothing(keys, values),
                                  control=list(warmup=10)) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%
      mean() %>%
      divide_by(1e6) %>%
      set_names(n1)
}
results <- data.frame(n=n, "no lookup"=time, check.names=FALSE)
time
```

We see that the theoretical lower bound is 
`r formatC(tail(results[["no lookup"]], 1), format="f",
   digits=0)`ms.


## Built-in `match`

R has built-in `match` function whose task is do do just such lookup.
We loop over the keys in randomized order and use match to find the
location of the corresponding key in the table.

```{r matchIntToInt, dependson="R"}
doMatch <- function(keys, values) {
   ## find values, corresponding to keys
   for(key in keys) {
      v <- values[match(key, keys)]
   }
}

time <- foreach(n1 = n, .combine=c) %do% {
   values <- sample(1000000000L, n1)
   keys <- sample(values)
                           # keys are values in random order
   microbenchmark::microbenchmark(doMatch(sample(keys), values), times=2L,
                                  control=list(warmup=1)) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%  # extract time in nanoseconds
      mean() %>%
      divide_by(1e6) %>%
      set_names(n1)
}
results[["match int-int"]] <- time
time
```
The result--`r formatC(tail(results[["match int-int"]], 1)/1000,
   format="f", digits=0)`s 
is not impressive but may work for many practical applications for 
`r tail(n,1)`-element table.



### Named vectors

R has a version of lookup table built-in in the form of named
vectors.  We test the speed in two ways: converting numeric keys to
character on the fly by using the construct
`values[as.character(key)]`; and using character keys where we convert
the keys in advance to a character vector, and later do just
`values[key]`.  Note that the latter may not be possible, in case of
integer keys.  However, quite often is is a feasible approach.

The on the fly conversion ("named vectors 1") gives the following timings:
```{r namedVector1, dependson="R"}
doNV1 <- function(keys, values) {
   ## find values, corresponding to keys
   for(key in keys) {
      v <- values[as.character(key)]
   }
}

time <- foreach(n1 = n, .combine=c) %do% {
   values <- sample(1000000000L, n1)
   keys <- sample(values)
                           # keys are values in random order
   values <- setNames(values, keys)
                           # create named vector keys/values
   microbenchmark::microbenchmark(doNV1(sample(keys), values), times=1L) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%
      mean() %>%
      divide_by(1e6) %>%
      set_names(n1)
}
results[["named vectors 1"]] <- time
time
```

And the pre-conversion ("named vectors 2") results in:
```{r namedVector2, dependson="R"}
doNV2 <- function(keys, values) {
   ## find values, corresponding to keys
   for(key in keys) {
      v <- values[keys]
   }
}

time <- foreach(n1 = n, .combine=c) %do% {
   values <- sample(1000000000L, n1)
   keys <- as.character(sample(values))
                           # keys are values in random order, a character strings
   values <- setNames(values, keys)
                           # create named vector keys/values
   microbenchmark::microbenchmark(doNV2(sample(keys), values), times=1L) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%
      mean() %>%
      divide_by(1e6) %>%
      set_names(n1)
}
results[["named vectors 2"]] <- time
time
```

As one can clearly see, both approaches are very slow.  Neither 
`r formatC(tail(results[["named vectors 1"]], 1)/1000, format="f", digits=0)`s nor 
`r formatC(tail(results[["named vectors 2"]], 1)/1000, format="f", digits=0)`s are feasible speeds
for a lookup of table of size `r tail(n,1)`.



### `fmatch` in "fastmatch"

Unlike built-in `match` with it's sequential lookup,
`fastmatch::fmatch` constructs the lookup hashtable at the first iteration, and later
uses it.  It results in dramatically better times for long tables.

```{r fmatchIntToInt, dependson="R"}
doFMatch <- function(keys, values) {
   ## find values, corresponding to keys
   for(key in keys) {
      v <- values[fastmatch::fmatch(key, keys)]
   }
}

time <- foreach(n1 = n, .combine=c) %do% {
   values <- sample(1000000000L, n1)
   keys <- sample(values)
                           # keys are values in random order
   microbenchmark::microbenchmark(doFMatch(sample(keys), values), times=2L,
                                  control=list(warmup=1)) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%  # extract time in nanoseconds
      mean() %>%
      divide_by(1e6) %>%
      set_names(n1)
}
results[["fmatch int-int"]] <- time
time
```
The result--`r formatC(tail(results[["fmatch int-int"]], 1)/1000,
   format="f", digits=1)`s--seems to be fair speed, potentially for
   most applications.


### Hashed environment

R environments can be used as built-in hashed lookup tables, as long
as the keys can be represented as character strings.  This works well
for integers.

```{r hEnv, dependson="R"}
doHEnv <- function(keys, values) {
   ## find values, corresponding to keys
   for(key in keys) {
      v <- env[[key]]
   }
}
time <- foreach(n1 = n, .combine=c) %do% {
   values <- sample(1000000000L, n1)
   keys <- as.character(sample(values))
                           # we need character values to access the environment
   values <- setNames(values, keys)
                           # make it into a vector...
   env <- list2env(as.list(values), hash=TRUE)
                           # and convert into a hashed environment
   microbenchmark::microbenchmark(doHEnv(sample(keys), values), times=2L,
                                  control=list(warmup=1)) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%  # extract time in nanoseconds
      mean() %>%
      divide_by(1e6) %>%
      set_names(n1)
}
results[["hashed env"]] <- time
time
```

So far, hashed environments appear to be the fastest approach.
Although the performance gap between this and `fmatch` decreases at
large n, with `r formatC(tail(results[["hashed env"]], 1)/1000,
   format="f", digits=1)`s of running time it is still noticeably faster
   than `fmatch`.


### Hashed envirnoment with on-the-fly-conversion


```{r hEnvInt, dependson="R"}
doHEnv <- function(keys, values) {
   ## find values, corresponding to keys
   for(key in keys) {
      v <- env[[as.character(key)]]
   }
}
time <- foreach(n1 = n, .combine=c) %do% {
   values <- sample(1000000000L, n1)
   keys <- sample(values)
                           # we need character values to access the environment
   values <- setNames(values, keys)
                           # make it into a vector...
   env <- list2env(as.list(values), hash=TRUE)
                           # and convert into a hashed environment
   microbenchmark::microbenchmark(doHEnv(sample(keys), values), times=2L,
                                  control=list(warmup=1)) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%  # extract time in nanoseconds
      mean() %>%
      divide_by(1e6) %>%
      set_names(n1)
}
results[["hashed fly"]] <- time
time
```

### Another benchmark: do the same in python

We benchmark the R approaches with the most straightforward python
approach: using a dict.

```{python, cache=TRUE}
import time
import random
import pandas as pd
ns = [10, 30, 100, 300, 1000, 3000, 10000, 30000, 100000, 300000]
t = []
for n in ns:
    values = [i for i in range(n)]
    keys = values.copy()
    random.shuffle(keys)
    table = dict(zip(values, keys))
    t0 = time.time()
    for k in keys:
        v = table[k]
    t.append((time.time() - t0)*1000)
    # in milliseconds
df = pd.DataFrame({ "n" : ns, "time" : t })
df.to_csv("python_results.csv", index=False)
print(df)
```


# Final plot

Finally, here is a plot with all approaches together.

```{r plotAll, echo=FALSE, cache=FALSE, fig.width=12, fig.height=12, comment=NA}
pythonResults <- read.csv("python_results.csv") %>%
   cbind(method = "python dict")
                           # cannot delete the file: if python chunk is taken from cache, it will not re-create it
tidyr::gather(results, key=method, value=time, -n) %>%
   rbind(pythonResults) %>%
   ggplot(aes(n, time, col=method)) +
   geom_line() + geom_point() +
   scale_x_log10() + scale_y_log10() +
   labs(y = "time, ms")
```

We see that indexing by names is always the slowest option.  Of the
tested R methods, the clear
winner is the hashed environment, at least up to the size of hundreds
of thousands.  The second place depends on the table size: built-in
`match` gives the best performance for up 10,000, for
larger tables `fastmatch::fmatch` is better.  However, python manages
to complete dictionary lookup almost at the speed where R just loops
through the values with no lookup.  Part of it is probably related to
less looping overhead in case of python, but dicts are also superior to any
of the R methods tested here.
