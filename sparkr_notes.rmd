---
title: "Taming Spark and SparkR"
subtitle: "While Waiting for Better Documentation"
author: Ott Toomet
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: 
  html_document:
    highlight: tango
    df_print: kable
    fig_caption: false
    toc: true
    toc_float:
      smooth_scroll: false
    toc_depth: 2
    number_sections: true
---

```{r setup, include=FALSE, cache=FALSE}
options(tibble.width=60, tibble.print_max=7, tibble.print_min=4)
knitr::opts_knit$set(aliases=c(h="fig.height"))
knitr::opts_chunk$set(fig.height=100/25.4, 
                      cache=TRUE,
                      message=FALSE)
library("SparkR", lib.loc=file.path(Sys.getenv("HOME"), "local", "spark", "R", "lib"))
library(magrittr)
ss <- sparkR.session(master = "local[2]",
                     appName = "sparkNotes",
                     sparkConfig=list(spark.sql.parquet.compression.codec="gzip",
                                      spark.driver.memory="8g",
                                      log4j.logger.org.apache.parquet="INFO"
                                      )
                     )
setLogLevel("WARN")
```

# Introduction

SparkR is a very valuable approach when working with big data.
Unfortunately it's documentation is rather sparse.  Here I have
collected a number of solutions and explanations I have been
struggling myself and hope others may find useful.


# General concepts

## RDD

The central data structure in spark is _Resilient Distributed Dataset_
(RDD).  Resilient means it can be recovered using an earlier stage and
the _RDD lineage_ information.  

So RDD is not just dataset: a number in RDD is not just a number, but
a number plus information where this number comes from.


### Checkpointing

As one operates repeatedly on the same data, the RDD lineage may grow
without limits.  This happens, for instance, if you add data to an RDD
in a loop, and each time also filter out certain observations.  This
manifests with RDD operations getting slower and slower, one may also get
`java.lang.StackOverflowError`-s.  A very valuable resource for
seeing and debugging the RDD lineages is the web monitoring port
4040 (the default value).  

A way to _break the lineage_ is to use _checkpointing_.  Checkpointing
saves the current dataframe as "just data", without any historical
information.  All the following execution plans take the resulting
dataframe as a blank sheet (well, blank in terms of lineage, not data
content!). 

Checkpointing can be done like this:
```{r, eval=FALSE}
setCheckpointDir("/tmp")
                           # note: have to set checkpoint dir
df2 <- checkpoint(df1)
```
Now `df2` is a lineage-less dataframe.



# Configuration

## Getting Run-Time Configuration

The basic run-time configuration can be returned by `sparkR.conf()`.
Without any arguments it returns all set options:
```{r}
sparkR.conf()
```
Note that there is no obvious entry for parallelism.  When invoked
locally, it is visible in the for of "master[2]", but no separate
entry is provided (2.2.1).


## Compression Codecs

By default, spark compresses parquet files by snappy codec which
creates rather large temporary files.  You have to configure it explicitly to use something else, such as "gzip".  The codec must
be specificed during the spark session initialization through _sparkConfig_
argument.  _sparkConfig_ is a list of configuration options in the
form of _name_ = _value_ and name for parquet compression is
_spark.sql.parquet.compression.codec_.  Value must be a string.  For instance:

```r
ss <- sparkR.session(master = "local[8]",
                     appName = "mySparkApp",
                     sparkConfig=list(spark.sql.parquet.compression.codec="gzip")
                     )
```

Note: this must be done when spark session is initialized, later calls
to `sparkR.conf` do not work.  Neither can you configure spark before
you have initialized the sparkSession.


## Java heap memory

Spark driver memory can be set when creating the spark session with
the `spark.driver.memory` option.  It takes character values like `"4g"`:

```{r javaHeapMemory, eval=FALSE}
ss <- sparkR.session(master = "local[2]",
                     sparkConfig=list(spark.driver.memory="8g")
                     )
```
If this is too small, you get java out-of-memory errors related to
java heap space.



## Temporary Directories

Spark stores quite a bit of temporary data on disk so you should
ensure you have plenty of disk space.  The location of temporary
folders can be set in the environment variable `SPARK_LOCAL_DIRS` as a
comma-separated list of directories:
```bash
SPARK_LOCAL_DIRS="$HOME/tmp,/tmp"
```
This creates temporary folders in two locations, one in `$HOME/tmp` and another in
`/tmp`.  The folders appear in pairs, names like
`blockmgr-6e392c2c...` and `spark-f0e4221e...`.

Spark splits data to be saved roughly equally onto both of these
devices.  This means, in particular, that when one of the devices is
full, spark fails even if there is still plenty of space on the
other (2.1.2).


# Data Input

## How to read csv files

CSV files are one of the most common and versatile forms of data.
SparkR reads these pretty well but unfortunately, the documentation is
not good enough to get started.  The basic syntax is:

```{R loadDF, eval=FALSE}
loadDF(fName,
       source="csv",
       header="true",
       sep=",")
```

The most important arguments are:

* `fName`:  the file name.  The unix home directory marker `~` does
  not seem to work (it is taken as a literal character in the file
  name), unlike in ordinary R.  However, there are a number of good
  news:
    * compressed files, at least `.bz2`,  are decompressed
      transparently (2.2.0).
	* `fName` can be a shell pattern corresponding to multiple files,
	  such as `dir/*.csv`.
	  In that case all these files are read and stacked
      into the resulting data frame.  
	* `fName` can also be the folder's name (both with and
      without the trailing slash) in which case all
      files in that folder are read.  Note: in this case `loadDF` loads all
      the files in that folder, including non-csv ones.
* `source`: should be "csv" here
* `header`: whether the file contains the first line as column names.
  Defaults to "false" which reads the eventual header names as the
  first line, and names columns to `_c0`, `_c1` etc. (2.2.0).  Note:
  you have to specify java-ish `"true"` and `"false"` (in quotes) here, not R-ish
  `TRUE` and `FALSE`.
* `sep`: field separator, default ",".
* there are more arguments.


`loadDF` loads everything as string type, you may want to convert the
columns afterwards (this can probably also be achieved by schemas).

There is an alternative function, `read.df`, that differs from loadDF
by the default arguments.


## Parquet files

```{R read.parquet, eval=FALSE}
read.parquet(file)
```

* `file`: file name, actually the name of the folder where parquet
  holds it's files
    * If you want to read all parquet file (or rather folders) in a
      directory, you have to specify `file` as `path/to/directory/*`.
      If you leave out the trailing wildcard `*`, you get
      an error _Unable to infer schema for Parquet_ (2.3.1).


# General operations

## Accessing columns

One can access single columns with the `$`-operator and the `column`
function.  The `$`-operator works in a fairly similar manner than for
ordinary data frames.  Note that in case of piping where your
intermediate dataframe does not have a name, you may access the
columns with `.$` way like `.$foo`.

Function `column` is an alternative, it takes column name as a
character: `column("foo")`.


# Merging 

## Merging and joining

SparkR features two similar functions: `merge` and `join`.  They do
not behave in quite the same manner.

Merge syntax is rather similar to that of base::merge:
```{r, eval=FALSE}
merge(x, y, by, suffixes)
```

* `x`, `y` are data frame to be merged.
* `by` is a vector of key columns, by default columns with similar
  name in both data frames.  
    * `by` columns can be of different type, for instance integer and
      double columns still match.
* `suffixes` is a character vector of 2 specifying different suffixes
  for `x` and `y` to make unique names.
    * Even if you want to retain just one version of the variable with
      it's original name, you cannot specify something like
      `suffices=c("", "_y")`.  You get an error.  (2.2.0)


### Merge renames variables (2.2.0)

If merging two data frames on a column of similar name, `merge` creates new variables
like `foo_x` and `foo_y` according to the respective origin data
frame.  For instance:
```{r mergeRenames}
foo <- createDataFrame(data.frame(a=1:2, b=11:12, c=c("a", "b")))
bar <- createDataFrame(data.frame(a=1:2, b=21:22, d=c("x", "y")))
merge(foo, bar, by="a") %>% collect()
```
If, instead of `by`, you specify both `by.x` and `by.y`, then it also
renames the `b`-s:
```{r mergeRenames2, dependson="mergeRenames"}
merge(foo, bar, by.x="a", by.y="a") %>% collect()
```

Join, in turn, does not rename but still keeps both version of the join column in the data:
```{r joinDoesntRename, dependson="mergeRenames"}
join(foo, bar, foo$a == bar$a) %>% collect()
```

### Merging and "trivial" join conditions

As stressed above, RDD is not just data but also information about how
the data was created.  `merge` can use some of this lineage information.



## Merging and lazy evaluation

Spark's lazy evaluation makes debugging hard, the code may fail
before it even reaches the problematic line.

For instance, when ordering before merging where the merge introduces
ambiguous columns you may see an error like: 

```
org.apache.spark.sql.AnalysisException: Reference 'b' is ambiguous, could be: b#3, b#10.;
```

For instance:
```{r mergeFails, eval=FALSE}
foo <- createDataFrame(data.frame(a=1:2, b=11:12))
bar <- createDataFrame(data.frame(a=1:2, b=21:22))
foo %>%
   groupBy("a", "b") %>%
   count() %>%
   merge(bar, by="a") %>%
   rename(a = column("a_x")) %>%
   collect()
```

fails with error `Caused by: org.apache.spark.sql.AnalysisException:
Reference 'b' is ambiguous, could be: b#3, b#10.;`.  The problem is
multiple instance of `b` introduced through merge, but failure seems
to occur
at `groupBy("a", "b")`.

However, a slightly simpler plan---just removing the `rename` part
that seems unrelated to the problem---still works and demonstrates what
happens at merge:
```{r mergeWorks}
foo <- createDataFrame(data.frame(a=1:2, b=11:12))
bar <- createDataFrame(data.frame(a=1:2, b=21:22))
foo %>%
   groupBy("a", "b") %>%
   count() %>%
   merge(bar, by="a") %>%
   collect()
```
By removing `rename(a = column("a_x"))` at the end of
the previous example the code works, I guess this is related to how
well can the code translated to spark execution plans.

(Spark 2.2.1)


# Ordering by Group

Many common tasks require ordering by group.  SparkR has multi-column
ordering function (`arrange`) and multi-column grouping (`groupBy`).
Unfortunately, these two do not play together.  Assume we have
temperature data
```{r temperatureData, cache=FALSE}
temp <- data.frame(month = c("Nov", "Nov", "Dec", "Dec"), day=c(1,15,1,15), temp=c(14,12,10,12)) %>%
   createDataFrame()
temp %>%
   collect()
```
and we want to find the hottest day each month.  Unlike in `dplyr`, you
cannot just do
```{r, eval=FALSE}
temp %>%
   groupBy("month") %>%
   arrange("temp") %>%
   agg(maxDay = day[1])
```
as grouped data cannot be arranged in spark.

## Window Functions

A correct solution seems to be to use window functions.  Windows in
SQL parlance are blocks of rows where one can do certain operations.
Windows can be defined in a different ways, here we are interested
partitioning data by keys into windows.  This can be done as
`windowPartitionBy("month")`.  This tells spark that data must be
"partitioned" by month (I guess the 'partition' here not the same as
RDD partition), and we can operate separately on these partitions.

Now we want to order the partitions by temperature.  This can be
achieved by `ws <- orderBy(windowPartitionBy("month"), "temp")`.  Note we
first define window partition, and thereafter it's ordering.  These
operations form our _window specification_ `ws`.  This is just
specification, we haven't touched the data yet.

When done with window specification, one can use window functions.
These can be invoked by `over(x, window)` where `x` is a column,
mostly likely computed via window functions, and `window` is the
window spec we defined above.  To solve the task above, we can add the
row number (i.e. temperature rank) for each day in month.  So we can
do:
```{r}
ws <- orderBy(windowPartitionBy("month"), "temp")
temp %>%
   mutate(n = over(row_number(), ws)) %>%
   filter(column("n") == 1) %>%
   collect()
```
(note the syntax `column("n")` to refer to the column "n").

This approach works otherwise great, but unfortunately it picks the
_lowest_ temperature day.  I haven't found a way to insert `desc`
operator for `orderBy` sorting--it only accepts string column names,
not `column("temp")`, no `desc("temp")`.  So we have to resort on a
stupid trick by reversing the sign of temp:
```{r}
ws <- orderBy(windowPartitionBy("month"), "temp1")
temp %>%
   mutate(temp1 = -column("temp")) %>%
   mutate(n = over(row_number(), ws)) %>%
   filter(column("n") == 1) %>%
   collect()
```
Obviously, in the final version we may drop obsolete variables and
order the data according to month.


## `arrange` - `groupBy`

Note: it is tempting to do go the following way instead:
```{r}
temp %>%
   arrange(column("month"), desc(column("temp"))) %>%
   groupBy("month") %>%
   agg(maxDay = first("day")) %>%
   collect()
```
This is simple, and seems to work, at least locally.  Unfortunately there is no
guarantee that `groupBy` preserves the order (see
[SPARK-16207](https://issues.apache.org/jira/browse/SPARK-16207)).



# How to Rename Columns

There are two built-in functions for renaming.  `rename` expects a
list of arguments in the form of `newName = existingColumn` (after the
data frame argument).  Note: this is quoted or unquoted name = column,
the right hand side must not be just
a column name.  This is
perhaps the easiest way to rename variables:
```{r}
data <- createDataFrame(data.frame(a=1:2, b=c("Huang", "Goyal")))
data %>%
   rename(id = column("a"), "name" = column("b")) %>%
   collect()
```

`withColumnRenamed` expects two arguments (after the data frame
argument): old column name and new column name.  Note: these are
column names, not columns, and the order is the other way around:
```{r, cache=FALSE}
data <- createDataFrame(data.frame(a=1:2, b=c("Huang", "Goyal")))
data %>%
   withColumnRenamed("a", "id") %>%
   withColumnRenamed("b", "name") %>%
   collect()
```

However, certain short column names do not work (2.2.1) and exit with
an uninformative error message:
```{r}
data %>%
   rename("x" = column("a")) %>%
   collect()
```

Name "xx" works, however:
```{r}
data %>%
   rename("xx" = column("a")) %>%
   collect()
```


# arrange

Spark dataframes can be ordered by `arrange` function.  It orders the
data by "natural order", i.e. alphabetically for character columns and
in numeric order for numerics.
The syntax is
similar to that of _dplyr_: `arrange(col, col, ...)`.  However, there
are two separate functions:
```r
arrange("col1", "col2", ...)
```
and
```r
arrange(col1, col2, ...)
```
In the former case, `arrange` expects a list of column names, in the
latter case a list of columns.
The latter version also works with order-reversal function `desc`, the
former version does not.
As an example, use the temperature data from above:
```{r}
temp %>%
   arrange("month", "temp") %>%
   collect()
temp %>%
   arrange(column("month"), desc(column("temp"))) %>%
   collect()
```
One cannot mix the two approaches: `arrange("x", column("y"))` will
not work

(2.2.1)


# Handling Dates

## Timestamps

It is not immediately obvious how to handle dates inside spark.  In
CSV files the
date data is usually in some sort of _yyyy-mm-dd_ form which will be
read as character string.  The spark internal format of choice would
be _utc\_timestamp_, the timestamp in UTC time zone.  The way there
may be slightly circuituous:

1. Convert the character representation (say, column "date") into
_unix\_timestamp_ with
```r
unix_timestamp(column("date"), "MM/dd/yyyy hh:mm:ss")
```
Note: spark expects [java date formatting](https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html), not `strptime` date
formatting. 

2. As _to\_utc\_timestamp_ we will use below expects _timestamp_ class, not
_unix\_timestamp_, one has to cast it into _timestamp_:
```r
cast(column("date"), "timestamp")
```

3. Finally, we can convert the column into _utc\_timestamp_ using the
correct timezone for the original data:
```r
to_utc_timestamp(column("date"), "AFT")
                           # use AFT time zone
```
**Warning**: if the time zone acronym is unknown, it will be silently ignored. 

An example script that performs all these tasks:
```{r dateConvertExample1
}
data <- createDataFrame(data.frame(date=c("3/22 2014 18:44:33", "5/3 2018 11:04:11")))
data2 <- data %>%
   withColumn("date", unix_timestamp(column("date"), "MM/dd yyyy HH:mm:ss")) %>%
   withColumn("date", cast(column("date"), "timestamp")) %>%
   withColumn("date", to_utc_timestamp(column("date"), "Asia/Kabul"))
data2 %>%
   collect()
```
Indeed, UTC time is less than Kabul time by 4 hours 30 mins.

The above can also be achieved in a more compact manner as:
```{r dateConvertExample2, dependson="dateConvertExample1"}
data %>%
   withColumn("date", unix_timestamp(column("date"), "MM/dd yyyy HH:mm:ss") %>%
                      cast("timestamp") %>%
                      to_utc_timestamp("Asia/Kabul")
              ) %>%
   collect()
```

(spark 2.1.2)

## Formatting dates

When we want to convert a _utc\_timestamp_ back to string representation in UTC, we
can use `date_format(column, format string)` (note: we need 
[java format symbols](https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html) again).
For instance:
```{r dateFormatExample1, dependson="dateConvertExample1"}
data2 %>%
   withColumn("date2", date_format(column("date"), "yyyyMMdd")) %>%
   collect()
```

Alternatively, one can achieve a similar result with extractor
functions `year`, `month`, `dayofmonth`, `hour`, `minute`, `second`.  For
instance: 
```{r dateFormatExample2, dependson="dateConvertExample1"}
data2 %>%
   withColumn("date2", year(column("date"))*10000 +
                       month(column("date"))*100 +
                       dayofmonth(column("date"))) %>%
   collect()
```
**Warning**: you must put the column in the first position in `withColumn`
arithmetic expression.  A literal number in the first position, such
as `10000*year(column("date"))` gives
an error.

(spark 2.2.0)


# String Operations

## startsWith, endsWith

These are utility functions that test if a string column starts or
ends with a given substring.  These return a logical column.

```{r startsWith, eval=FALSE}
startsWith(column, substring)
endsWith(column, substring)
```

Example:
```{r endsWithExample}
df <- data.frame(x=c("foo", "bar")) %>%
   createDataFrame()
df %>%
   withColumn("y", endsWith(column("x"), "oo")) %>%
   collect()
```


# other notes (draft)

## Web Monitoring Ports

if you start spark locally (--master local[n]), then the 8080 and 8081
ports are not there, only 4040.



## Merging by a large number of keys

during merges (and possibly sorts) you may end up with being out of
memory with error message like

java.lang.IllegalArgumentException: Cannot allocate a page with more
than 17179869176 bytes

This is claimed to be related to grouping by key.  Spark tries to
allocate equal amount of memory for each key, and if the distribution
is skewed, it attempts to allocate too much.


## finding NA-s

I cannot understand how is.nan works.  Use instead

isNull(col)

or

isNotNull(col)

they can be used for filtering and they mark NA-s as TRUE


## User-Defined Functions

when doing UDF-s and return data.frame, you may get "Unsupported type
for serialization" error.  This may be related to the fact that
returned data.frame converts strings to factors and sparkR cannot
serialize factors.  Use stringsAsFactors=FALSE


## Parquet file for temporary data

you cannot easily overwrite parquet file with new data.  The old cache
remains in memory and it causes errors.


## creating empty spark dataframe

empty R data.frame %>% createDataFrame gives out-of-bounds errors


## Reading Data

read.parquet(): expects a list of files as the first argument
read.df() expects a path where to read all the files as the first
argument

(spark 2.2)


## pivot

don't pivot on groupBy columns


## dapply

the chunks fed to dapply are roungly of the size \#obs/\#partitions. nParallel
dapply processes are run in parallel.

If you get error message `error in parallel:::mcfork unable to create
a pipe` then you may have run out of processes.  Spark 2.1.2 and R
3.5.0 did not play nicely with dapply leaving zombies, until you run
out of the available processes.
In this case fewer partitions may help.  Issue seems to be gone when
switching to spark 2.3.1.

One runs a separate dapply
process for each partition?  At least the number of dapply invocations
seems to
equal the number of partitions.


```{r finalize, include=FALSE}
sparkR.stop()
```
