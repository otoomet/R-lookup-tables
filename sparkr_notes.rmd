---
title: "Taming Spark and SparkR"
subtitle: "While Waiting for Better Documentation"
author: Ott Toomet
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: 
  html_document:
    highlight: tango
    df_print: kable
    fig_caption: false
---

```{r setup, include=FALSE, cache=FALSE}
options(tibble.width=60, tibble.print_max=7, tibble.print_min=4)
knitr::opts_knit$set(aliases=c(h="fig.height"))
knitr::opts_chunk$set(fig.height=100/25.4, 
                      cache=TRUE,
                      message=FALSE)
library("SparkR", lib.loc=file.path(Sys.getenv("HOME"), "local", "spark", "R", "lib"))
library(magrittr)
ss <- sparkR.session(master = "local[2]",
                     appName = "sparkNotes",
                     sparkConfig=list(spark.sql.parquet.compression.codec="gzip",
                                      spark.driver.memory="8g",
                                      log4j.logger.org.apache.parquet="INFO"
                                      )
                     )
setLogLevel("WARN")
```

# Introduction

SparkR is a very valuable approach to big data handling.
Unfortunately it's documentation is rather sparse.  Here I have
collected a number of solutions and explanations I have been
struggling myself and hope others may find useful.


# Compression Codecs

By default, spark compresses parquet files by snappy codec which
creates rather large temporary files.  You have
to configure it to use something else, such as "gzip".  The codec must
be specificed when initiatin spark session through _sparkConfig_
argument.  _sparkConfig_ is a list of configuration options in the
form of _name_ = _value_ and name for parquet compression is
_spark.sql.parquet.comression.codec_.  Value must be a string.  For instance:

```r
ss <- sparkR.session(master = "local[8]",
                     appName = "mySparkApp",
                     sparkConfig=list(spark.sql.parquet.compression.codec="gzip")
                     )
```

Note: this must be done when spark session is initialized, later calls
to `sparkR.conf` do not work.  Neither can you configure spark before
you have initialized the sparkSession.


# Merging and Lazy Evaluation

Spark's lazy evaluation makes debugging hard, the code may fail
before it even reaches the problematic line.

For instance, when ordering before merging where the merge introduces
ambiguous columns you may see an error like: 

```
org.apache.spark.sql.AnalysisException: Reference 'b' is ambiguous, could be: b#3, b#10.;
```

For instance:
```{r, eval=FALSE}
foo <- createDataFrame(data.frame(a=1:2, b=11:12))
bar <- createDataFrame(data.frame(a=1:2, b=21:22))
foobar <- foo %>%
   groupBy("a", "b") %>%
   count() %>%
   merge(bar, by="a") %>%
   rename(a = column("a_x"))
```

fails with error `Caused by: org.apache.spark.sql.AnalysisException:
Reference 'b' is ambiguous, could be: b#3, b#10.;`.  The problem is
multiple instance of `b` introduced through merge, but failure occurs
at `groupBy("a", "b")`.

However, a slightly simpler plan still works and demonstrates what
happens at merge:
```{r}
foo <- createDataFrame(data.frame(a=1:2, b=11:12))
bar <- createDataFrame(data.frame(a=1:2, b=21:22))
foo %>%
   groupBy("a", "b") %>%
   count() %>%
   merge(bar, by="a") %>%
   collect()
```
By removing `rename(a = column("a_x"))` at the end of
the previous example the code works, I guess this is related to how
well can the code translated to spark execution plans.

(Spark 2.2.1)


# Ordering by Group

Many common tasks require ordering by group.  SparkR has multi-column
ordering function (`arrange`) and multi-column grouping (`groupBy`).
Unfortunately, these two do not play together.  Assume we have
temperature data
```{r, cache=FALSE}
temp <- data.frame(month = c("Nov", "Nov", "Dec", "Dec"), day=c(1,15,1,15), temp=c(14,12,10,12)) %>%
   createDataFrame()
temp %>%
   collect()
```
and we want to find the hottest day each month.  Unlike in `dplyr`, you
cannot just do
```{r, eval=FALSE}
temp %>%
   groupBy("month") %>%
   arrange("temp") %>%
   agg(maxDay = day[1])
```
as grouped data cannot be arranged in spark.

## Window Functions

A correct solution seems to be to use window functions.  Windows in
SQL parlance are blocks of rows where one can do certain operations.
Windows can be defined in a different ways, here we are interested
partitioning data by keys into windows.  This can be done as
`windowPartitionBy("month")`.  This tells spark that data must be
"partitioned" by month (I guess the 'partition' here not the same as
RDD partition), and we can operate separately on these partitions.

Now we want to order the partitions by temperature.  This can be
achieved by `ws <- orderBy(windowPartitionBy("month"), "temp")`.  Note we
first define window partition, and thereafter it's ordering.  These
operations form our _window specification_ `ws`.  This is just
specification, we haven't touched the data yet.

When done with window specification, one can use window functions.
These can be invoked by `over(x, window)` where `x` is a column,
mostly likely computed via window functions, and `window` is the
window spec we defined above.  To solve the task above, we can add the
row number (i.e. temperature rank) for each day in month.  So we can
do:
```{r}
ws <- orderBy(windowPartitionBy("month"), "temp")
temp %>%
   mutate(n = over(row_number(), ws)) %>%
   filter(column("n") == 1) %>%
   collect()
```
(note the syntax `column("n")` to refer to the column "n").

This approach works otherwise great, but unfortunately it picks the
_lowest_ temperature day.  I haven't found a way to insert `desc`
operator for `orderBy` sorting--it only accepts string column names,
not `column("temp")`, no `desc("temp")`.  So we have to resort on a
stupid trick by reversing the sign of temp:
```{r}
ws <- orderBy(windowPartitionBy("month"), "temp1")
temp %>%
   mutate(temp1 = -column("temp")) %>%
   mutate(n = over(row_number(), ws)) %>%
   filter(column("n") == 1) %>%
   collect()
```
Obviously, in the final version we may drop obsolete variables and
order the data according to month.


## `arrange` - `groupBy`

Note: it is tempting to do go the following way instead:
```{r}
temp %>%
   arrange(column("month"), desc(column("temp"))) %>%
   groupBy("month") %>%
   agg(maxDay = first("day")) %>%
   collect()
```
This is simple, and seems to work, at least locally.  Unfortunately there is no
guarantee that `groupBy` preserves the order (see
[SPARK-16207](https://issues.apache.org/jira/browse/SPARK-16207)).



# other notes (incomplete)

## Web Monitoring Ports

if you start spark locally (--master local[n]), then the 8080 and 8081
ports are not there, only 4040.



## Merging by a large number of keys

during merges (and possibly sorts) you may end up with being out of
memory with error message like

java.lang.IllegalArgumentException: Cannot allocate a page with more
than 17179869176 bytes

This is claimed to be related to grouping by key.  Spark tries to
allocate equal amount of memory for each key, and if the distribution
is skewed, it attempts to allocate too much.


## finding NA-s

I cannot understand how is.nan works.  Use instead

isNull(col)

or

isNotNull(col)

they can be used for filtering and they mark NA-s as TRUE


## User-Defined Functions

when doing UDF-s and return data.frame, you may get "Unsupported type
for serialization" error.  This may be related to the fact that
returned data.frame converts strings to factors and sparkR cannot
serialize factors.  Use stringsAsFactors=FALSE


## Parquet file for temporary data

you cannot easily overwrite parquet file with new data.  The old cache
remains in memory and it causes errors.


## creating empty spark dataframe

empty R data.frame %>% createDataFrame gives out-of-bounds errors


## RDD lineage

java.lang.StackOverflowError may be caused by rdd lineage getting too
long 


## Reading Data

read.parquet(): expects a list of files as the first argument
read.df() expects a path where to read all the files as the first
argument

(spark 2.2)


## reading csv files

loadDF(fName, source="csv", header="true", sep=",")
   default: no header, source not csv, sep=','

(2.2.1)

## Timestamps

unix\_timestamp(column, format) gives 'date' class
to\_utc\_timestamp() expects 'timestamp' class
you have to cast unix\_timestamp() to 'timestamp'

(spark 2.2.1)


## rename

needs meaningful colnames? (2.2.1)



## pivot

don't pivot on groupBy columns


## arrange

arrange("x", "y")

or 

arrange(column("x"), column("y"))  (works with 'desc')

but not 

arrange("x", column("y"))




```{r finalize, include=FALSE}
sparkR.stop()
```

