---
title: "Notes on SparkR"
subtitle: "While Waiting for Better Documentation"
author: Ott Toomet
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: 
  html_document:
    highlight: tango
    df_print: kable
    fig_caption: false
---

```{r setup, include=FALSE, cache=FALSE}
options(tibble.width=60, tibble.print_max=7, tibble.print_min=4)
knitr::opts_knit$set(aliases=c(h="fig.height"))
knitr::opts_chunk$set(fig.height=100/25.4, 
                      cache=TRUE,
                      message=FALSE)
library("SparkR", lib.loc=file.path(Sys.getenv("HOME"), "local", "spark", "R", "lib"))
ss <- sparkR.session(master = "local[2]",
                     appName = "sparkNotes",
                     sparkConfig=list(spark.sql.parquet.compression.codec="gzip",
                                      spark.driver.memory="8g",
                                      log4j.logger.org.apache.parquet="INFO"
                                      )
                     )
setLogLevel("WARN")
```

# Introduction

SparkR is a very valuable approach to big data handling.
Unfortunately it's documentation is rather sparse.  Here I have
collected a number of solutions and explanations I have been
struggling myself and hope others may find useful.


# Compression Codecs

By default, spark compresses parquet files by snappy codec which
creates rather large temporary files.  You have
to configure it to use something else, such as "gzip".  The codec must
be specificed when initiatin spark session through _sparkConfig_
argument.  _sparkConfig_ is a list of configuration options in the
form of _name_ = _value_ and name for parquet compression is
_spark.sql.parquet.comression.codec_.  Value must be a string.  For instance:

```r
ss <- sparkR.session(master = "local[8]",
                     appName = "mySparkApp",
                     sparkConfig=list(spark.sql.parquet.compression.codec="gzip")
                     )
```

Note: this must be done when spark session is initialized, later calls
to `sparkR.conf` do not work.  Neither can you configure spark before
you have initialized the sparkSession.


# Merging and Lazy Evaluation

Spark's lazy evaluation makes debugging hard, the code may fail
before it even reaches the problematic line.

For instance, when ordering before merging where the merge introduces
ambiguous columns you may see an error like: 

```
org.apache.spark.sql.AnalysisException: Reference 'b' is ambiguous, could be: b#3, b#10.;
```

For instance:
```{r, eval=FALSE}
foo <- createDataFrame(data.frame(a=1:2, b=11:12))
bar <- createDataFrame(data.frame(a=1:2, b=21:22))
foobar <- foo %>%
   groupBy("a", "b") %>%
   count() %>%
   merge(bar, by="a") %>%
   rename(a = column("a_x"))
```

fails with error `Caused by: org.apache.spark.sql.AnalysisException:
Reference 'b' is ambiguous, could be: b#3, b#10.;`.  The problem is
multiple instance of `b` introduced through merge, but failure occurs
at `groupBy("a", "b")`.

However, a slightly simpler plan still works and demonstrates what
happens at merge:
```{r}
foo <- createDataFrame(data.frame(a=1:2, b=11:12))
bar <- createDataFrame(data.frame(a=1:2, b=21:22))
foo %>%
   groupBy("a", "b") %>%
   count() %>%
   merge(bar, by="a") %>%
   collect()
```
By removing `rename(a = column("a_x"))` at the end of
the previous example the code works, I guess this is related to how
well can the code translated to spark execution plans.

(Spark 2.2.1)

# other notes (incomplete)

## Web Monitoring Ports

if you start spark locally (--master local[n]), then the 8080 and 8081
ports are not there, only 4040.



## Merging by a large number of keys

during merges (and possibly sorts) you may end up with being out of
memory with error message like

java.lang.IllegalArgumentException: Cannot allocate a page with more
than 17179869176 bytes

This is claimed to be related to grouping by key.  Spark tries to
allocate equal amount of memory for each key, and if the distribution
is skewed, it attempts to allocate too much.


## finding NA-s

I cannot understand how is.nan works.  Use instead

isNull(col)

or

isNotNull(col)

they can be used for filtering and they mark NA-s as TRUE


## User-Defined Functions

when doing UDF-s and return data.frame, you may get "Unsupported type
for serialization" error.  This may be related to the fact that
returned data.frame converts strings to factors and sparkR cannot
serialize factors.  Use stringsAsFactors=FALSE


## Parquet file for temporary data

you cannot easily overwrite parquet file with new data.  The old cache
remains in memory and it causes errors.


## creating empty spark dataframe

empty R data.frame %>% createDataFrame gives out-of-bounds errors


## RDD lineage

java.lang.StackOverflowError may be caused by rdd lineage getting too
long 


## Reading Data

read.parquet(): expects a list of files as the first argument
read.df() expects a path where to read all the files as the first
argument

(spark 2.2)


## reading csv files

loadDF(fName, source="csv", header="true", sep=",")
   default: no header, source not csv, sep=','

(2.2.1)

## Timestamps

unix\_timestamp(column, format) gives 'date' class
to\_utc\_timestamp() expects 'timestamp' class
you have to cast unix\_timestamp() to 'timestamp'

(spark 2.2.1)


## library dependencies

sparkR imports magrittr (2.2.1)


## rename

needs meaningful colnames? (2.2.1)




```{r finalize, include=FALSE}
sparkR.stop()
```

