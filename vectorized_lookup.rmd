---
title: "Vectorized Int-to-Int Value Lookup R"
subtitle: "Different Approaches"
author: Ott Toomet
date: "`r format(Sys.time(), '%Y-%m-%d')`"
output: 
  html_document:
    highlight: tango
    df_print: kable
    fig_caption: false
    code_folding: hide
---

# Setup

In [a sister post](https://otoomet.github.io/simple_lookup_R.html)
I compare lookup-up time in integer-key dictionaries in R, and
benchmark it with a few basic python approaches.  Here I essentially repeat the same test,
but instead of one-by-one lookup I am extracting values corresponding
to a vector of keys.

The tests are done on
`r system("lscpu | grep 'Model name:' | sed -e 's/Model name: \\+//'",
intern=TRUE)` on a single core.

I create a vector of keys and values and use the first one to look up the
value in the second one.  I test several different data structures to
store these, and several related lookup methods.


```{r setup, echo=FALSE}
knitr::opts_knit$set(aliases=c(h="fig.height"))
knitr::opts_chunk$set(fig.height=100/25.4, 
                     message=FALSE,
                     cache=TRUE,
                     cache.path=".vectorized_lookup_cache/",
                     engine.path = list(python = "/usr/bin/python3")
                     )
library(foreach)  # for convenient loops
library(magrittr) # 'cause I love pipes
library(microbenchmark) # to avoid delay when using the first time
library(data.table)
library(ggplot2)
```

```{r R}
L <- 10
                           # number of look-ups
B <- 10000
                           # key vector size for a single lookup
ns <- c(3e1, 1e2, 3e2, 1e3, 3e3, 1e4, 3e4, 1e5, 3e5, 1e6)
                           # timing repetitions used later
```

Here I am accessing only a few
times--`r L` times--a long vectors of values--`r B` values from tables
of different sizes.  This goes toward the other extreme compared to
the individual lookup in the sister post.  The table sizes are
```{r, depends="R"}
ns
```

We use _microbenchmark_ library and report timing in milliseconds.


## Benchmark: no lookup

First we do the exercise with no actual value lookup.  This serves as
a benchmark: in particular for small lookup tables, the actual lookup
time may be small compared to other related overheads.  We just loop
over all keys and pick the value corresponding to that key, using
positional indexing.

```{r noLookup, dependson="R"}
doNothing <- function(sKeys, values) {
   ## find values, corresponding to keys
   for(key in sKeys) {
      v <- values[key]
   }
}
time <- foreach(n = ns, .combine=c) %do% {
   values <- sample(1000000000L, n)
   keys <- sample(seq(along=values))
                           # keys are just the integer indices, in random order
   shuffledKeys <- foreach(l = seq(length=L)) %do% {
      sample(keys, B, replace=TRUE)
   }
                           # a list of L index vectors of size B
   microbenchmark(doNothing(shuffledKeys, values),
                                  times=10L,
                                  control=list(warmup=10)) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%
      mean() %>%
      divide_by(1e6) %>%
      set_names(n)
}
results <- data.frame(n=ns, "no lookup"=time, check.names=FALSE)
time
```

We see that the theoretical lower bound is 
`r formatC(tail(time, 1), format="f",
   digits=2)`ms.  This time does not depend on the the lookup table
size as we use plain integer indexing.


## Built-in `match`

R has built-in `match` function whose task is to do just such lookup.
As it takes vector arguments, we can just loop over the key vectors:

```{r match, dependson="R"}
doMatch <- function(sKeys, keys, values) {
   ## find values, corresponding to keys
   for(key in sKeys) {
      v <- values[match(key, keys)]
   }
}

timeMatch <- foreach(n = ns, .combine=c) %do% {
   values <- sample(1000000000L, n)
   keys <- sample(values)
                           # keys are values in random order
   shuffledKeys <- foreach(l = seq(length=L)) %do% {
      sample(keys, B, replace=TRUE)
   }
                           # a list of L index vectors of size B
   microbenchmark::microbenchmark(doMatch(shuffledKeys, keys, values), times=2L,
                                  control=list(warmup=1)) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%  # extract time in nanoseconds
      mean() %>%
      divide_by(1e6) %>%
      set_names(n)
}
timeMatch
```
The result--`r formatC(tail(timeMatch, 1)/1000,
   format="f", digits=3)`s 
is not impressive but will probably work for many practical applications for 
`r tail(ns,1)`-element table.


### Named vectors

R has a version of lookup table built-in in the form of named
vectors.  This also accepts vectors of names to look up. 
We test the speed by assigning the keys as names to the
value vector, and thereafter we convert numeric keys to
character during the lookup with
`values[as.character(key)]`.  Note that in many applications one may
also be able to pre-convert all the keys to characters.

We get the following timings:
```{r namedVector, dependson="R"}
doNV1 <- function(sKeys, values) {
   ## find values, corresponding to keys
   for(key in sKeys) {
      v <- values[as.character(key)]
   }
}
timeNV <- foreach(n = ns, .combine=c) %do% {
   values <- sample(1000000000L, n)
   keys <- sample(values)
                           # keys are values in random order
   values <- setNames(values, keys)
                           # create named vector keys/values
   shuffledKeys <- foreach(l = seq(length=L)) %do% {
      sample(keys, B, replace=TRUE)
   }
                           # a list of L index vectors of size B
   microbenchmark::microbenchmark(doNV1(shuffledKeys, values), times=1L) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%
      mean() %>%
      divide_by(1e6) %>%
      set_names(n)
}
resultsNV <- data.frame(n=ns, time=timeNV, method="named vectors", check.names=FALSE)
timeNV
```

As one can clearly see, the approach is noticeably slower but
`r formatC(tail(timeNV, 1)/1000, format="f",
digits=3)`s for `r tail(ns,1)` keys may still be acceptable in practice.



### `fmatch` in "fastmatch"

Unlike built-in `match` with it's sequential lookup,
`fastmatch::fmatch` builds the lookup hashtable at the first iteration, and later
uses it.  It is developed as a drop-in replacement for `match` and can
result in dramatically better times for long tables.

```{r fmatch, dependson="R"}
doFMatch <- function(sKeys, keys, values) {
   ## find values, corresponding to keys
   for(key in sKeys) {
      v <- values[fastmatch::fmatch(key, keys)]
   }
}

time <- foreach(n = ns, .combine=c) %do% {
   values <- sample(1000000000L, ns)
   keys <- sample(values)
                           # keys are values in random order
   shuffledKeys <- foreach(l = seq(length=L)) %do% {
      sample(keys, B, replace=TRUE)
   }
                           # a list of L index vectors of size B
   microbenchmark::microbenchmark(doFMatch(shuffledKeys, keys, values), times=2L,
                                  control=list(warmup=1)) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%  # extract time in nanoseconds
      mean() %>%
      divide_by(1e6) %>%
      set_names(n)
}
results[["fmatch"]] <- time
time
```
The result is `r formatC(tail(results[["fmatch"]], 1),
   format="f", digits=3)` _ms_ --i.e. 
`r formatC(tail(results[["fmatch"]], 1)/tail(ns, 1)*1e6,
   format="f", digits=3)` _ns_ per lookup seems rather good and
suggests that `fmatch` is also rather cache-friendly.


### Hashed environment

For single-value lookup, hashed environments--the closest R offers for
hashmaps--[provide the best
performance](https://otoomet.github.io/simple_lookup_R.html).  Unfortunately these do not support vectorized access, so
we have to loop through the vector of keys.  Note also
that this requires the keys to be represented as character vector.
This works well
for integers.  We create a vector of values, assign the keys to it as
names, convert the named vector to a list, and finally into a hashed
environment with `list2env(., hash=TRUE)`.  We perform the lookup in
the environment with
`values[[key]]` where keys are converted to characters earlier in vectorized
fashion. 


```{r hEnvInt, dependson="R"}
doHEnv <- function(sKeys, values) {
   ## find values, corresponding to keys
   for(bKeys in sKeys) {
                           # extract the vector of bunch keys
      for(key in as.character(bKeys)) {
         v <- values[[key]]
      }
   }
}
time <- foreach(n = ns, .combine=c) %do% {
   values <- sample(1000000000L, n)
   keys <- sample(values)
                           # we need character values to access the environment
   values <- setNames(values, keys) %>%
                           # make it into a vector...
      as.list() %>%
      list2env(hash=TRUE)
                           # and convert into a hashed environment
   shuffledKeys <- foreach(l = seq(length=L)) %do% {
      sample(keys, B, replace=TRUE)
   }
                           # a list of L index vectors of size B
   microbenchmark::microbenchmark(doHEnv(shuffledKeys, values), times=2L,
                                  control=list(warmup=1)) %>%
                           # find keys in random order
      as.data.frame() %>%
      extract2("time") %>%  # extract time in nanoseconds
      mean() %>%
      divide_by(1e6) %>%
      set_names(n)
}
results[["hashed env"]] <- time
time
```

Unlike single results, hashed environments appear slow.  At the
largest table, the time
`r formatC(tail(results[["hashed env"]], 1)/1000, 
   format="f", digits=3)`s is about 1000x slower than `fmatch`, and
even slower than named vectors--the definitive looser when testing
the single-key lookups.
   

### Data.tables

data.table library offers an alternative to data frame and contains
built-in ordering by index.  We use the vectorized index (as key) lookup of a the value from data.table

```{r data.table, dependson="R"}
doDT <- function(sKeys, dtable) {
   ## find values, corresponding to keys
   for(key in sKeys) {
      v <- dtable[key, values]
   }
}
time <- foreach(n = ns, .combine=c) %do% {
   values <- sample(1000000000L, n)
   keys <- sample(values)
                           # we need character values to access the environment
   dtable <- data.table(keys, values, key="keys")
                           # set hashed index to the table
   shuffledKeys <- foreach(l = seq(length=L)) %do% {
      sample(keys, B, replace=TRUE)
   }
                           # a list of L index vectors of size B
   microbenchmark::microbenchmark(doDT(shuffledKeys, dtable), times=3L,
                                  control=list(warmup=2)) %>%
      as.data.frame() %>%
      extract2("time") %>%  # extract time in nanoseconds
      mean() %>%
      divide_by(1e6) %>%
      set_names(n)
}
results[["data.table"]] <- time
time
```

Unlike in case of single-key lookup, data tables appear quite fast.
Their access time appears to be remarkably constant over different
sizes.  Clearly, these offer a good alternative for `fmatch`, at least
in case of multi-value lookups.  The huge performance gain for
vectorized lookup compared to the single-index case
suggests that `[.data.table` has quite substantial overhead.


### Another benchmark: do the same in python

Finally, let's leave R and use the standard dict in python.  Python dict 

```{python, cache=TRUE}
import time
import random
import pandas as pd
L = 10
B = 10000
ns = [30, 100, 300, 1000, 3000, 10000, 30000, 100000, 300000, 1000000]
td = []
for n in ns:
    values = [i for i in range(n)]
    keys = values.copy()
    random.shuffle(keys)
    table = dict(zip(values, keys))
    lk = [[random.choice(keys) for _ in range(B)]
           for _ in range(L)]
    # list of list of keys to be looked up
    t0 = time.time()
    # start the clock
    for bKeys in lk:
        # take the bunch
        for k in bKeys:
            # take individual keys in bunches
            v = table[k]
    td.append((time.time() - t0)*1000)
    # in milliseconds
## Now repeat with pandas
tp = []
for n in ns:
    values = [i for i in range(n)]
    keys = values.copy()
    random.shuffle(keys)
    #
    table = pd.DataFrame({ "values" : values}, index=keys)
    lk = [[random.choice(keys) for _ in range(B)]
           for _ in range(L)]
    # list of list of keys to be looked up
    t0 = time.time()
    # start the clock
    for bKeys in lk:
        # take the bunch
        v = table.loc[bKeys]
        # index by vector of keys
    tp.append((time.time() - t0)*1000)
df = pd.DataFrame({ "n" : ns, "dict" : td , "pandas" : tp})
df.to_csv("python_results.csv", index=False)
print(df)
```
Python dict appears to be outperformed by several R methods
here.  Looping to address the non-vectorized lookup only is probably
the main culprit.  It also appears that plain indexed lookup in pandas
does even worse.


# Conclusions and the final plot

Finally, here is a plot with all approaches together.

```{r plotAll, echo=FALSE, cache=FALSE, fig.width=12, fig.height=10, comment=NA}
results[["match"]] <- timeMatch
pythonResults <- read.csv("python_results.csv") %>%
   tidyr::gather(key=method, value=time, -n)
                           # cannot delete the file: if python chunk is taken from cache, it will not re-create it
tidyr::gather(results, key=method, value=time, -n) %>%
   rbind(resultsNV) %>%
   rbind(pythonResults) %>%
   ggplot(aes(n, time, col=method)) +
   geom_line() + geom_point() +
   scale_x_log10() + scale_y_log10() +
   labs(title = paste("Time to look up", L, "keys from dictionary"),
        y = "time, ms",
        x = "dictionary size") +
   theme(text = element_text(size=14))
```

We see that `fmatch` is clearly the best performer over all table
sizes.  As it is very easy to use and it performs well for
one-element lookups too, this seems to be the best overall performer for R
integer key lookups.  Data tables also offer stable and reasonably good
performance.  The worst performers are named vectors--probably
because of their sequential search approach, and hashed environments
as these do not offer vectorized lookup.

As a surprise, the basic python methods do not perform well.  Dicts
that offer by far superior performance for single-element lookup are
slow now--probably because the element lookup is not
vectorized at C level.  And pandas, also offering index-based
hashtable, are even slower.  It suggests that under the hood the index
lookup is not vectorized either.  As the aim of this post is to
compare various R methods and benchmark these with the simple python
ways, I do not asses the question about existence of superior methods
in python.
